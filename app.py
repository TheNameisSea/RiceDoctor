#_____Imports_____#
# Timm
import sys
sys.path.append('../input/pytorch-image-models/pytorch-image-models-master')

# Settings
import warnings
import sklearn.exceptions
warnings.filterwarnings('ignore', category=DeprecationWarning)
warnings.filterwarnings('ignore', category=FutureWarning)
warnings.filterwarnings("ignore", category=sklearn.exceptions.UndefinedMetricWarning)

# General
from collections import defaultdict
import pandas as pd
import numpy as np
import os, shutil
import random
import gc
import cv2
from PIL import Image
import glob
gc.enable()
pd.set_option('display.max_columns', None)

# Augmentations
import albumentations
from albumentations.pytorch.transforms import ToTensorV2

# Deep Learning
from torch.utils.data import Dataset, DataLoader
import torch
import torchvision
import timm
import torch.nn as nn
import torch.nn.functional as F

# UI
import streamlit as st

# Seeds
RANDOM_SEED = 42

def seed_everything(seed=RANDOM_SEED):
    os.environ['PYTHONHASHSEED'] = str(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True
    
seed_everything()

# Device
device = torch.device('cpu')

#_____Data_____#
# required datafile
data_dir = './input'        # image folder
models_dir = './models'     # model folder
test_df = pd.DataFrame()

id2label = {0: 'Báº¡c lÃ¡',
            1: 'Äá»‘m sá»c lÃ¡',
            2: 'Báº¡c chuá»³ lÃ¡',
            3: 'ChÃ¡y lÃºa',
            4: 'Äá»‘m nÃ¢u',
            5: 'SÃ¢u Ä‘á»¥c thÃ¢n lÃºa',
            6: 'SÆ°Æ¡ng mai',
            7: 'Bá» gai',
            8: 'BÃ¬nh thÆ°á»ng',
            9: 'Vi-rÃºt Tungro',
            10: 'KhÃ´ng xÃ¡c Ä‘á»‹nh'}

bacla = "Xá»­ lÃ½ bá»‡nh báº¡c lÃ¡:\n Sá»­ dá»¥ng cÃ¡c thuá»‘c trong danh má»¥c thuá»‘c BVTV Ä‘Æ°á»£c phÃ©p sá»­ dá»¥ng á»Ÿ Viá»‡t Nam cÃ³ chá»©a cÃ¡c hoáº¡t cháº¥t Bismerthiazol, Copper hydroxide, Oxolinic acid, Thiodiazole zinc, Thiodiazole copper,â€¦ Ä‘á»ƒ phun.\nGiai Ä‘oáº¡n lÃºa Ä‘Ã²ng â€“ trá»• â€“ chÃ­n, bÃ  con cáº§n theo dÃµi cháº·t cháº½ diá»…n biáº¿n cá»§a thá»i tiáº¿t tiáº¿n hÃ nh phun trÆ°á»›c vÃ  sau mÆ°a giÃ´ng báº±ng cÃ¡c thuá»‘c báº£o vá»‡ thá»±c váº­t cÃ³ chá»©a hoáº¡t cháº¥t nÃªu trÃªn theo nguyÃªn táº¯c 4 Ä‘Ãºng vÃ  theo hÆ°á»›ng dáº«n sá»­ dá»¥ng trÃªn bao bÃ¬ Ä‘á»ƒ ngÄƒn cháº·n bá»‡nh lÃ¢y lan trÃªn diá»‡n rá»™ng."
domsocla = "Xá»­ lÃ½ bá»‡nh Ä‘á»‘m sá»c lÃ¡:\nKhÃ´ng bÃ³n quÃ¡ nhiá»u Ä‘áº¡m, bÃ³n Ä‘áº¡m muá»™n vÃ  kÃ©o dÃ i; ChÃº Ã½ káº¿t há»£p giá»¯a bÃ³n Ä‘áº¡m vá»›i phÃ¢n chuá»“ng, lÃ¢n, kali.\nKhi phÃ¡t hiá»‡n tháº¥y ruá»™ng chá»›m bá»‹ bá»‡nh cáº§n giá»¯ má»±c nÆ°á»›c tá»« 3 - 5 cm, tuyá»‡t Ä‘á»‘i khÃ´ng Ä‘Æ°á»£c bÃ³n cÃ¡c loáº¡i phÃ¢n hÃ³a há»c, phÃ¢n bÃ³n qua lÃ¡ vÃ  thuá»‘c kÃ­ch thÃ­ch sinh trÆ°á»Ÿng.\n Sá»­ dá»¥ng cÃ¡c loáº¡i thuá»‘c hoÃ¡ há»c Ä‘á»ƒ phun phÃ²ng trá»« nhÆ°: Totan 200WP, Starwiner 20WP, Novaba 68WP, Xanthomix 20WP, PN - Balacide 32 WP, Starner 20 WP,... pha vÃ  phun theo hÆ°á»›ng dáº«n trÃªn vá» bao bÃ¬." 
bacchuyla = "Xá»­ lÃ½ bá»‡nh báº¡c chuá»³ lÃ¡:\nChá»n giá»‘ng lÃºa Ã­t nhiá»…m bá»‡nh, lÃ m Ä‘áº¥t ká»¹, giáº£i Ä‘á»™c há»¯u cÆ¡, sáº¡ thÆ°a, bÃ³n phÃ¢n cÃ¢n Ä‘á»‘i NPK, bÃ³n Ä‘áº§y Ä‘á»§ canxi, silic, kali, khÃ´ng Ä‘á»ƒ nÆ°á»›c ngáº­p sÃ¢u kÃ©o dÃ i.\n VÃ o giai Ä‘oáº¡n cuá»‘i Ä‘áº» nhÃ¡nh, nuÃ´i Ä‘Ã²ng náº¿u xuáº¥t hiá»‡n bá»‡nh ChÃ¡y bÃ¬a lÃ¡ do vi khuáº©n cÃ³ thá»ƒ sá»­ dá»¥ng Agrilife 100SL vá»›i liá»u 30ml/25 lÃ­t.\nVÃ o giai Ä‘oáº¡n lÃºa trá»• Ä‘áº¿n chÃ­n: phun thuá»‘c 25ml Agrilife 100SL +25ml Keviar 325SC/25 lÃ­t 2 láº§n: lÃºc lÃºa trá»• láº¹t xáº¹t vÃ  lÃºc lÃºa trá»• Ä‘á»u."
chaylua = "Xá»­ lÃ½ bá»‡nh chÃ¡y lÃºa:\nTrÆ°á»›c khi trá»“ng vá»¥ má»›i thÃ¬ tiáº¿n hÃ nh cáº£i táº¡o vÃ  vá»‡ sinh sáº¡ch Ä‘á»“ng ruá»™ng Ä‘á»ƒ trÃ¡nh máº§m khuáº©n gÃ¢y bá»‡nh.\nBÃ³n phÃ¢n cÃ¢n Ä‘á»‘i dá»±a trÃªn mÃ u lÃ¡ lÃºa, chÃº Ã½ giá»¯ má»±c nÆ°á»›c trong ruá»™ng thÃ­ch há»£p tá»« 5-10cm so vá»›i cÃ¢y lÃºa.\nKhi má»›i phÃ¡t hiá»‡n bá»‡nh, nÃªn rÃºt nÆ°á»›c trong ruá»™ng ra rá»“i tiáº¿n hÃ nh ráº£i vÃ´i vá»›i liá»u lÆ°á»£ng 10 â€“ 20kg/1.000m2.\nSá»­ dá»¥ng thuá»‘c Ä‘áº·c trá»‹ bá»‡nh chÃ¡y lÃ¡ lÃºa khi cáº§n thiáº¿t, sá»­ dá»¥ng theo hÆ°á»›ng dáº«n cá»§a nhÃ  sáº£n xuáº¥t trÃªn bao bÃ¬."
domnau = "Xá»­ lÃ½ bá»‡nh Ä‘á»‘m nÃ¢u:\nXá»­ lÃ½ giá»‘ng trÆ°á»›c khi gieo báº±ng nÆ°á»›c nÃ³ng 54 Ä‘á»™ C hoáº·c báº±ng cÃ¡ch dÃ¹ng má»™t trong cÃ¡c loáº¡i thuá»‘c trá»« bá»‡nh nhÆ° Carban 50SC, Vicarben 50HP... pha ná»“ng Ä‘á»™ 3/1.000 ngÃ¢m giá»‘ng 24-36 giá» sau Ä‘Ã³ vá»›t ra Ä‘Ã£i sáº¡ch rá»“i Ä‘em á»§ bÃ¬nh thÆ°á»ng.\nSá»­ dá»¥ng má»™t trong cÃ¡c loáº¡i thuá»‘c nhÆ°: Kacie 250EC, Golcol 20SL, Supercin 20EC/40EC/80EC, Carbenzim 500FL, Tilt Super 300EC, Viroval 50BTN, Workup 9SL... Ä‘á»ƒ phun xá»‹t. Vá» liá»u lÆ°á»£ng vÃ  cÃ¡ch sá»­ dá»¥ng nÃªn Ä‘á»c ká»¹ hÆ°á»›ng dáº«n cá»§a nhÃ  sáº£n xuáº¥t cÃ³ in sáºµn trÃªn bao bÃ¬."
sauducthanlua = "Xá»­ lÃ½ bá»‡nh sÃ¢u Ä‘á»¥c thÃ¢n lÃºa:\n Sá»­ dá»¥ng sáº£n pháº©m TT Checker 270SC, liá»u lÆ°á»£ng 40ml/bÃ¬nh 25L.\nBá»• sung thÃªm cho cÃ¢y lÃºa cháº¥t Ä‘iá»u hÃ²a sinh trÆ°á»Ÿng nhÆ° Plastimula 1SL vá»›i liá»u lÆ°á»£ng 30ml/bÃ¬nh 25L á»Ÿ cÃ¡c thá»i ká»³ quan trá»ng nhÆ°: xá»­ lÃ½ giá»‘ng, Ä‘áº» nhÃ¡nh, lÃ m Ä‘Ã²ng."
suongmai = "Xá»­ lÃ½ bá»‡nh sÆ°Æ¡ng mai:\nSá»­ dá»¥ng cháº¿ pháº©m sinh há»c trá»« bá»‡nh RV04 vá»›i cÆ¡ cháº¿ tÃ¡c Ä‘á»™ng kÃ©p cá»§a náº¥m Ä‘á»‘i khÃ¡ng vÃ  Enzyme kÃ­ch khÃ¡ng cÃ¢y trá»“ng tiÃªu diá»‡t, cÃ´ láº­p cÃ¡c váº¿t bá»‡nh do náº¥m vÃ  vi khuáº©n táº¥n cÃ´ng, ngÄƒn cháº·n ká»‹p thá»i sá»± lÃ¢y lan cá»§a máº§m bá»‡nh.\nDá»±a vÃ o Ä‘iá»u kiá»‡n thá»i tiáº¿t mÃ  nÃªn tham kháº£o sá»­ dá»¥ng Ä‘á»ƒ phÃ²ng bá»‡nh tá»« Ä‘áº§u."
bogai = "Xá»­ lÃ½ bá» gai\nLÃ m sáº¡ch cá» dáº¡i trong ruá»™ng vÃ  bá» bao.\nDiá»‡t trá»« sÃ¢u non trÃªn máº¡ mÃ¹a sáº¯p cáº¥y báº±ng cÃ¡ch ngáº¯t bá» cÃ¡c lÃ¡ bá»‹ háº¡i cÃ³ bá» gai.\nDÃ¹ng thuá»‘c trá»« sÃ¢u nhÃ³m lÃ¢n há»¯u cÆ¡, Carbamate hoáº·c CÃºc tá»•ng há»£p Ä‘á»u cÃ³ thá»ƒ diá»‡t Ä‘Æ°á»£c bá» gai trÆ°á»Ÿng thÃ nh vÃ  áº¥u trÃ¹ng."
tungro = "Xá»­ lÃ½ virus Tungro:\nPhun cÃ¡c loáº¡i thuá»‘c trá»« sÃ¢u cÃ³ gá»‘c buprofezin hay pymetrozine á»Ÿ thá»i Ä‘iá»ƒm ngÃ y thá»© 15 vÃ  ngÃ y thá»© 30 sau khi cáº¥y cÃ³ thá»ƒ Ä‘áº¡t hiá»‡u quáº£ náº¿u Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘Ãºng thá»i gian. CÃ¡c loÃ i cÃ¢y quanh cÃ¡nh Ä‘á»“ng cÅ©ng cáº§n Ä‘Æ°á»£c phun cÃ¡c loáº¡i thuá»‘c trá»« sÃ¢u nÃªu trÃªn.\nKhÃ´ng nÃªn sá»­ dá»¥ng cÃ¡c sáº£n pháº©m thuá»‘c trá»« sÃ¢u cÃ³ gá»‘c chlorpyriphos, lamda cyhalothrin hay cÃ¡c cÃ´ng thá»©c káº¿t há»£p cÃ¡c cháº¥t pyrethroid tá»•ng há»£p vÃ¬ cÃ¡c loÃ i sÃ¢u ráº§y Ä‘Ã£ pháº§n nÃ o Ä‘á» khÃ¡ng Ä‘Æ°á»£c cÃ¡c loáº¡i thuá»‘c áº¥y. "
binhthuong = "LÃºa khÃ´ng bá»‹ bá»‡nh hoáº·c khÃ´ng bá»‹ bá»‡nh do virus / vi khuáº©n / náº¥m"

# Params
params = {
    'model': 'efficientnet_b3',
    'pretrained': False,
    'inp_channels': 3,
    'im_size': 300,
    'device': device,
    'batch_size': 85,
    'num_workers' : 0,
    'out_features': 11,
    'dropout': 0.2,
    'num_fold': 10,
    'debug': False,
}

# Transform
def get_test_transforms(DIM = params['im_size']):
    return albumentations.Compose(
        [
          albumentations.Resize(DIM,DIM),
          albumentations.Normalize(
              mean=[0.485, 0.456, 0.406],
              std=[0.229, 0.224, 0.225],
          ),
          ToTensorV2(p=1.0)
        ]
    )

# Dataset and Dataloader
class PaddyDataset(Dataset):
    def __init__(self, images_filepaths, transform=None):
        self.images_filepaths = images_filepaths
        self.transform = transform

    def __len__(self):
        return len(self.images_filepaths)

    def __getitem__(self, idx):
        image_filepath = self.images_filepaths[idx]
        image = cv2.imread(image_filepath)
        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)

        if self.transform is not None:
            image = self.transform(image=image)['image']
        
        return image

#_____Deep Learning_____#
# Neural Net
class PaddyNet(nn.Module):
    def __init__(self, model_name=params['model'], out_features=params['out_features'], inp_channels=params['inp_channels'],
                 pretrained=params['pretrained']):
        super().__init__()
        self.model = timm.create_model(model_name, pretrained=pretrained, in_chans=inp_channels)
        out_channels = self.model.conv_stem.out_channels
        kernel_size = self.model.conv_stem.kernel_size
        stride = self.model.conv_stem.stride
        padding = self.model.conv_stem.padding
        bias = self.model.conv_stem.bias
        self.model.conv_stem = nn.Conv2d(inp_channels, out_channels,
                                          kernel_size=kernel_size, stride=stride,
                                          padding=padding, bias=bias)
        n_features = self.model.classifier.in_features
        self.model.classifier = nn.Identity()
        self.dropout = nn.Dropout(params['dropout'])
        self.fc = nn.Linear(n_features, out_features)
    
    def forward(self, image):
        embeddings = self.model(image)
        x = self.dropout(embeddings)
        output = self.fc(x)
        return output
    
# Utils
@st.cache_data
def load_image(image_file):
    img = Image.open(image_file)
    return img

# Page cfgs
st.set_page_config(page_title="Rice Disease Classification", page_icon="ğŸ”¬", layout="centered", initial_sidebar_state="expanded", menu_items=None) 

# Page Title

st.write("""
# BÃ¡c SÄ© LÃºa
Cháº©n Ä‘oÃ¡n bá»‡nh lÃºa dá»±a trÃªn hÃ¬nh áº£nh 
""")
st.write('[LÆ°u Ã½:]')
luu_y = [
    'Sáº£n pháº©m chá»‰ sá»­ dá»¥ng Ä‘Æ°á»£c cho lÃºa, xin vui lÃ²ng khÃ´ng nháº­p áº£nh cá»§a cÃ¡c váº­t khÃ¡c vÃ o app',
    'NhÃ  phÃ¡t triá»ƒn khÃ´ng Ä‘áº£m báº£o Ä‘á»™ chÃ­nh xÃ¡c cá»§a dá»± Ä‘oÃ¡n cho táº¥t cáº£ cÃ¡c dá»± Ä‘oÃ¡n dá»±a trÃªn áº£nh cá»§a cÃ¡c giá»‘ng cÃ¢y ngoÃ i lÃºa',
    'Chá»‰ cÃ³ tÃ¡c dá»¥ng vá»›i áº£nh chá»¥p tá»« Ä‘iá»‡n thoáº¡i (tá»‰ lá»‡ 4:3, Ä‘á»™ phÃ¢n giáº£i khuyáº¿n khÃ­ch: 1440x1080)'
]
st.write(luu_y)
        
st.sidebar.header('Táº£i áº£nh lÃªn Ä‘á»ƒ báº¯t Ä‘áº§u cháº©n Ä‘oÃ¡n')
uploaded_files = st.sidebar.file_uploader("Táº£i áº£nh", accept_multiple_files=True)

res_col, img_col = st.columns([4, 2])

if uploaded_files is not None:
    try: 
        if os.path.exists('./upload'):
            shutil.rmtree('./upload')
        os.mkdir('./upload')
        upload_path = "./upload"

        # os.chdir('./database')
        # for uploaded_file in uploaded_files:
        #     img = load_image(uploaded_file)
        #     with open(uploaded_file.name, "wb") as f:
        #         f.write(uploaded_file.getbuffer())
        # os.chdir('../')

        os.chdir(upload_path)
        for uploaded_file in uploaded_files:
            img = load_image(uploaded_file)
            with open(uploaded_file.name, "wb") as f:
                f.write(uploaded_file.getbuffer())
        os.chdir('../')

        image_ids = os.listdir(upload_path)      # take name of all uploaded file
        if '.DS_Store' in image_ids:
            image_ids.remove('.DS_Store')
        # folder_len = len(image_ids)     # Take number of uploaded file

        # test_df = test_df[(test_df.index < folder_len)]     # remove images that could cause overflow
        test_df['image_id'] = image_ids     # add name of uploaded file to template
        paths = test_df.apply(lambda row: './upload/' + row['image_id'], axis=1)
        paths = paths.to_numpy()
        test_df['image_path'] = paths    # create filepaths



        # Prediction
        pred_cols = []

        for i, model_name in enumerate(glob.glob(models_dir + '/*.pth')):
            model = PaddyNet()
            model.load_state_dict(torch.load(model_name, map_location=torch.device('cpu')))     # load model
            model = model.to(params['device'])
            model.eval()        # evaluate model for uses
            
            X_test = test_df['image_path']      # read filepaths

            # create a dataset of images and apply augmentation to the images
            test_dataset = PaddyDataset(
                images_filepaths=X_test.values,     
                transform = get_test_transforms()
            )
            
            # data loader is used to load data
            test_loader = DataLoader(
                test_dataset, batch_size=params['batch_size'],
                shuffle=False, num_workers=params['num_workers'],
                pin_memory=True
            )

            temp_preds = None
            with torch.no_grad():
                for images in test_loader:
                    images = images.to(params['device'], non_blocking=True)
                    predictions = model(images).softmax(dim=1).argmax(dim=1).to('cpu').numpy()
                    
                    if temp_preds is None:
                        temp_preds = predictions
                    else:
                        temp_preds = np.hstack((temp_preds, predictions))

            test_df[f'model_{i}_preds'] = temp_preds
            pred_cols.append(f'model_{i}_preds')

        test_df['label'] = test_df[pred_cols].mode(axis=1)[0]
        test_df = test_df[['image_id', 'label']]
        label_col = test_df['label'].copy()
        test_df['label'] = test_df['label'].map(id2label)

        with res_col:
            st.write(test_df)
            for i in label_col.unique():
                if i == 0:
                    st.write(bacla)
                elif i == 1:
                    st.write(domsocla)
                elif i == 2:
                    st.write(bacchuyla)
                elif i == 3:
                    st.write(chaylua)
                elif i == 4:
                    st.write(domnau)
                elif i == 5:
                    st.write(sauducthanlua)
                elif i == 6:
                    st.write(suongmai)
                elif i == 7:
                    st.write(bogai)
                elif i == 8:
                    st.write(binhthuong)
                elif i == 9:
                    st.write(tungro)  
                elif i == 10:
                    st.write('áº¢nh khÃ´ng xÃ¡c Ä‘á»‹nh (KhÃ´ng pháº£i áº£nh lÃºa / cháº¥t lÆ°á»£ng áº£nh khÃ´ng phÃ¹ há»£p)')   

        with img_col:
            for img_path in os.listdir(upload_path)[:2]:
                image = Image.open(os.path.join('./upload', img_path))
                st.image(image, use_column_width='auto')

    except:
        pass

        




